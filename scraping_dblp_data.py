# -*- coding: utf-8 -*-
"""scraping_dblp_data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NlqdCSVxuk_5lggWWgDWl5KRcKOdf9F3
"""

import csv
import json
import time
import requests
import random

# Define the DBLP API endpoint and query parameters
DBLP_API_ENDPOINT = "https://dblp.org/search/publ/api"
QUERY_TEMPLATE = "year:{} venue:{}"

# Define the CSV filenames
PAPERS_CSV_FILENAME = "papers.csv"
AUTHORS_CSV_FILENAME = "authors.csv"
# years = [2000, 2001, 2002]  # Define the years to loop over
# venues = ["MOBICOM", "ASPLOS", "ISCA"]  # Define the venues to loop over

years = [2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022]  # Define the years to loop over
venues = ["MOBICOM", "ASPLOS", "ISCA", "AAAI", "ISWC", "ESWC", "IJCAI", "CIKM", "ACL", "KDD", "ECCV", "ICCV", "CVPR", "NeurIPS", "WWW", "UbiComp", "SIGIR", "SIGCOMM", "IJCAR", "ICDE", "AAMAS", "ACMMM", "CAV", "CRYPTO", "HPCA", "FOGA", "ICDM"]  # Define the venues to loop over
# #AAAI$|ISWC$|ESWC$|IJCAI$|ICML$|EMNLP$|ICLR$|CIKM$|ACL$|KDD$|ECCV$|ICCV$|CVPR$|NeurIPS$|WWW$|UbiCompSIGIR$|SIGCOMM$|IJCAR$|ICDE$|AAMAS$|ACMMM$|CAV$|CRYPTO$|HPCA$|FOGA$|ICDM$|",

# Loop over the years and venues and write the publications to the CSV files
for year in years:
    for venue in venues:
        query = QUERY_TEMPLATE.format(year, venue)

        # Define the query parameters
        query_params = {
            "format": "json",
            "h": 1000,  # Set the number of results to 1000
            "q": query,
        }

        while True:
            try:
                # Send the API request and get the JSON response
                response = requests.get(DBLP_API_ENDPOINT, params=query_params)
                response.raise_for_status()
                print("Done with ", query)
                data = json.loads(response.text)

                # Check if the API returned a None object
                if data.get("result") is None:
                    break

                publications = data.get("result").get("hits").get("hit")
                if publications is None:
                    break
                # Loop over the publications and write them to the CSV files
                for pub in publications:
                    # Extract the publication information
                    if pub is None:
                        continue

                    paper_id = pub.get("info").get("key")
                    title = pub.get("info").get("title")

                    if pub.get("info").get("authors") is None:
                        continue

                    authors = pub.get("info").get("authors").get("author")

                    if isinstance(authors, list):
                        authors = [(author.get("pid"), author.get("text")) for author in authors]
                    elif isinstance(authors, dict):
                        authors = [(authors.get("pid"), authors.get("text"))]
                    elif isinstance(authors, str):
                        authors = [(None, authors)]

                    venue = pub.get("info").get("venue")

                    if isinstance(venue, list):
                        continue
                    elif isinstance(authors, str):
                        venue = venue

                    year = int(pub.get("info").get("year"))

                    # Write the publication to the papers CSV file
                    with open(PAPERS_CSV_FILENAME, "a", newline="", encoding="utf-8") as papers_csv_file:
                        papers_csv_writer = csv.writer(papers_csv_file)
                        papers_csv_writer.writerow([paper_id, title, venue or "", year])

                    # Write the authors to the authors CSV file
                    with open(AUTHORS_CSV_FILENAME, "a", newline="", encoding="utf-8") as authors_csv_file:
                        authors_csv_writer = csv.writer(authors_csv_file)
                        for author in authors:
                            author_id, author_name = author
                            authors_csv_writer.writerow([paper_id, author_name])

                break
            except (requests.exceptions.HTTPError, json.JSONDecodeError):
                print("An error occurred while retrieving data from the API. Retrying in 5 seconds...")
                time.sleep(5)